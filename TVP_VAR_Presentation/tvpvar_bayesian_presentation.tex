\scshape\documentclass[xcolor=svgnames]{beamer}

\usetheme{Berlin}

\usepackage{listings}
	\lstset{language=TeX}

\usepackage{graphicx}
\usepackage{textcomp}

\newcounter{ale}
\newcommand{\abc}{\item[\alph{ale})]\stepcounter{ale}}

\newcommand{\nin}{\noindent}
\newenvironment{liste}{\begin{itemize}}{\end{itemize}}
\newcommand{\aliste}{\begin{liste} \setcounter{ale}{1}}
\newcommand{\zliste}{\end{liste}}

\newenvironment{abcliste}{\aliste}{\zliste}


\setbeamertemplate{headline}
{%
  \begin{beamercolorbox}{section in head/foot}
    \vskip3pt \insertsectionnavigationhorizontal{1\linewidth}{ }{  }  \vskip4pt
  \end{beamercolorbox}%%
  \begin{beamercolorbox}{section in toc}
    \vskip3pt
  \end{beamercolorbox}%
}
\setbeamertemplate{footline}
{
	\begin{beamercolorbox}{section in toc}
    \vskip2pt
  \end{beamercolorbox}%
  \begin{beamercolorbox}{section in head/foot}
  \vskip4pt
  \hskip4pt Bayesian Presentation
  \hskip15pt \text{Thomas Stockwell}
  \hskip150pt Spring 2018
  \hskip10pt \insertframenumber
  \vskip4pt
    \end{beamercolorbox}
}
\setbeamertemplate{navigation symbols}{\insertbackfindforwardnavigationsymbol}

\setbeamertemplate{items}[default] 
\setbeamercolor{section in toc}{fg=Green, bg=Green}
\setbeamercolor{palette primary}{fg=black, bg=white}
\setbeamercolor{palette secondary}{fg=black, bg=white}
\setbeamercolor{palette tertiary}{fg=Black, bg=LightGrey}
\setbeamercolor{palette quaternary}{fg=white, bg=black}
\setbeamercolor{items}{fg=black, bg=Green}
\setbeamercolor{title}{fg=black, bg=white}
\setbeamercolor{item projected}{fg=black, bg=white}
%\usecolortheme{dove}
\setbeamercolor{normal text}{fg=black, bg=white}
\setbeamercolor{block title}{fg=black, bg=white}
\setbeamercolor{structure}{fg=black}
\usefonttheme{structuresmallcapsserif}
\setbeamerfont{title}{size=\Large,series=\mdseries,parent={block body}}


\author{Thomas Stockwell \\ \textit{}}
\title{Bayesian Estimation of a TVP-VAR Model}
\vskip10pt
\date{5/30/2018}

\newtheorem{quest}{Question}
\newtheorem{ans}{Answer}

\begin{document}
\frame{\titlepage}

%\section{Introduction}

\begin{frame}
\frametitle{Normal VAR Model}
\begin{itemize}
	\item  The normal VAR is expressed in this way:
	$$ Y_t =  Y_{t-1} \beta + \epsilon_t $$
	\item where:
	\begin{itemize}
		\item $Y_t$ is a Mx1 vector of our data
		\vspace{0.1 in}
		\item $Y_{t-1}$ is a Mxk matrix containing exogenous explanatory variables (contains lagged values of Y and the constant term)
		\vspace{0.1 in}
		\item $\beta$ is a Mx1 vector of parameters to be estimated
		\begin{itemize}
			\item Assumed to be constant over time
		\end{itemize}
		\vspace{0.1 in}
		\item $\epsilon_t$ is a Mx1 vector of disturbance terms
		\begin{itemize}
			\item Assume: $\epsilon_t \sim N(0,\Sigma)$ \
			\item $\Sigma$ is assumed constant over time
		\end{itemize}
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Shortcoming of a VAR Model}
\begin{itemize}
	\item  In practice, the VAR model may not be very practical
	\vspace{0.1 in}
	\item Its restrictive in two main ways:
	\begin{itemize}
		\item  The parameters of the model may vary over time
		\item  The variance of the exogenous shocks may vary over time
	\end{itemize}
	\vspace{0.1 in}
	\item  Example: US Monetary Policy and the high inflation and low growth in the 1970's
	\begin{itemize}
		\item  Changing parameters: The Fed may have changed its response to inflation during this time
		\item  Changing variance: The '70's had high volatility whereas later policy makers experienced the Great Moderation 
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The TVP-VAR Model}
\begin{itemize}
	\item  We can use the TVP-VAR to address both of these issues
	\vspace{0.1 in}
	\item  TVP: Time Varying Parameters
	\vspace{0.1 in}
	\item  We can allow the parameters of the model to change over time or the variance of the disturbance term (or both)
	\vspace{0.1 in}
	\item  We will assume that errors are homoskedastic for simplicity and only focus on the time varying parameters
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The TVP-VAR Model}
\begin{itemize}
	\item  The TVP-VAR is expressed in this way:
	$$ Y_t =  Y_{t-1} \beta_t + \epsilon_t $$
	\item We let the parameters evolve according to an AR(1) process:
	$$ \beta_t = \beta_{t-1} + u_t $$
	\item  All the variables have the same definitions that we saw in the normal VAR case
	\vspace{0.1 in}
	\item  The new variable is $u_t \sim N(0,Q)$
	\vspace{0.1 in}
	\item  Do you notice anything familiar about the setup of this model?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{The TVP-VAR Model}
\begin{itemize}
	\item  This is a state-space model:
	\begin{itemize}
		\item  The measurement equation is: $ Y_t =  Y_{t-1} \beta_t + \epsilon_t $
		\item  The transition equation is: $ \beta_t = \beta_{t-1} + u_t $
	\end{itemize}
	\vspace{0.1 in}
	\item  When we think of Bayesian estimation of this model, we can use the same techniques that we learned in class when estimating state-space models
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Bayesian Estimation}
\begin{itemize}
	\item  Our parameters for this model: $\Sigma$, Q
	\begin{itemize}
		\item  Well call these $\theta$
	\end{itemize}
	\vspace{0.1 in}
	\item Unknown objects of interest:
	\begin{itemize}
		\item  $\theta$
		\item  $\beta_t$
	\end{itemize}
	\vspace{0.1 in}
	\item Our goal is to sample from $P(\theta,\tilde{\beta}_T|\tilde{Y}_T)$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{MCMC Algorithm}
\begin{itemize}
	\item  We will use a 3-step Gibbs sampler to accomplish our goal:
	\begin{itemize}
		\item  Step 1: Draw $\Sigma$ from $P(\Sigma|\tilde{\beta}_T^{g-1},\tilde{Y}_T)$, gives us $\Sigma^g$
		\item  Step 2: Draw Q from $P(Q|\tilde{\beta}_T^{g-1},\tilde{Y}_T)$, gives us $Q^g$
		\item  Step 3: Draw $\tilde{\beta}_T$ from $P(\tilde{\beta}_T|\theta^g,\tilde{Y}_T)$, gives us $\tilde{\beta}_T^g$
	\end{itemize}
	\vspace{0.1 in}
	\item  Step 3 is exactly what Jeremy did in the state-space model notes
	\vspace{0.1 in}
	\item  Steps 1 and 2 are the ones we did not do in class, but the Koop and Korobilis book goes over this
	\begin{itemize}
		\item  Start with step 1:
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Sampling $P(\theta|\tilde{\beta}_T^{g-1},\tilde{Y}_T)$}
\begin{itemize}
	\item  For $\Sigma^{-1}$, our prior is a $Wishart(S,v)$
	\item  After combining this with the marginal likelihood, the conditional posterior is:
	\vspace{0.1 in}
		$$P(\Sigma^{-1}|\tilde{\beta}_T,\tilde{Y}_T) \sim Wishart(\bar{S}^{-1},\bar{v})$$ 
			$$\bar{v} = v + T$$
			$$\bar{S}^{-1} = S + \sum\limits_{t=2}^{T} (Y_t-Y_{t-1}^{'}\beta_t)(Y_t-Y_{t-1}^{'}\beta_t)'$$
	\begin{itemize}
		\item "v" and "S" are hyperparameters
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Sampling $P(\theta|\tilde{\beta}_T^{g-1},\tilde{Y}_T)$}
\begin{itemize}
	\item  For $Q^{-1}$, our prior is also a $Wishart(S_Q,v_Q)$
	\item  After combining this with the marginal likelihood, the conditional posterior is:
	\vspace{0.1 in}
		$$P(Q^{-1}|\tilde{\beta}_T,\tilde{Y}_T) \sim Wishart(\bar{S}_Q^{-1},\bar{v}_Q)$$ 
			$$\bar{v}_Q = v_Q + T$$
			$$\bar{S}_Q^{-1} = S_Q + \sum\limits_{t=1}^{T} (\beta_{t+1}-I\beta_t)(\beta_{t+1}-I\beta_t)'$$
	\begin{itemize}
		\item "$v_Q$" and "$S_Q$" are hyperparameters
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Sampling $P(\tilde{\beta}_T|\theta^g,\tilde{Y}_T)$}
\begin{itemize}
	\item  To sample this distribution, we will follow the Carter and Kohn (1994) that we did in the state-space model notes
	\vspace{0.1 in}
	$$ P(\tilde{\beta}_T|\theta,\tilde{Y}_T) = P(\beta_1,...,\beta_T|\theta,\tilde{Y}_T) $$
	$$ = P(\beta_1|\beta_2,...,\beta_T,\theta,\tilde{Y}_T) ... P(\beta_{T-1}|\beta_T,\theta,\tilde{Y}_T)P(\beta_T|\theta,\tilde{Y}_T) $$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Sampling $P(\tilde{\beta}_T|\theta^g,\tilde{Y}_T)$}
\begin{itemize}
	\item  Recall that $\beta_t$ is an AR(1) process with an iid disturbance vector, therefore:
	\begin{itemize}
		\item  Given $\beta_{t+1}$, the pdf of $\beta_t$ does not depend on $\beta_{t+2},...,\beta_T$
		\item  Conditional on $\tilde{Y}_T$, then $Y_{t+1},...,Y_T$ contains no additional information about $\beta_t$
	\end{itemize}
	\vspace{0.1 in}
	\item This means that:
	$$P(\tilde{\beta}_T|\theta,\tilde{Y}_T)=P(\beta_1|\beta_2,\theta,\tilde{Y}_1)...P(\beta_{T-1}|\beta_T,\theta,\tilde{Y}_{T-1})P(\beta_T|\theta,\tilde{Y}_T)$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Sampling $P(\tilde{\beta}_T|\theta^g,\tilde{Y}_T)$}
\begin{itemize}
	\item  This means we can draw from $P(\tilde{\beta}_T|\theta,\tilde{Y}_T)$ as follows:
	\begin{itemize}
		\item  Step 1: Draw $\beta_T$ from $P(\beta_T|\theta,\tilde{Y}_T)$
		\item  Step 2: Draw $\beta_{T-1}$ from $P(\beta_{T-1}|\beta_T,\theta,\tilde{Y}_{T-1})$
	\end{itemize}
	\vspace{0.1 in}
	$$ \vdots $$
	\begin{itemize}
		\item  Step T: Draw $\beta_1$ from $P(\beta_1|\beta_2,\theta,\tilde{Y}_1)$
	\end{itemize}
	\vspace{0.1 in}
	\item If we need it, we can also draw $\beta_0$ from $P(\beta_0|\beta_1,\theta,\tilde{Y}_0)$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Sampling $P(\tilde{\beta}_T|\theta^g,\tilde{Y}_T)$}
\begin{itemize}
	\item  How do we take these draws?
	\item  From our time-series class, we know that:	
		$$ P(\beta_t|\theta,\tilde{Y}_t)=N(\beta_{tt},V_{tt}) $$
		$$ \beta_{tt}=E(\beta_t|\theta,\tilde{Y}_t) $$
		$$ V_{tt}=Var(\beta_t|\theta,\tilde{Y}_t) $$
		
	\item  $\beta_{tt}$ and $V_{tt}$ we get from the Kalman Filter
	\begin{itemize}
		\item  Therefore, we can get $\beta_{TT}$ and $V_{TT}$ from the final step of the Kalman Filter and then draw $X_T$ from $N(\beta_{TT},V_{TT})$
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Sampling $P(\tilde{\beta}_T|\theta^g,\tilde{Y}_T)$}
\begin{itemize}
	\item  What about the rest of the steps 2-T?
	$$ P(\beta_t|\beta_{t+1},\theta,\tilde{Y}_t)=N(E(\beta_t|\beta_{t+1},\theta,\tilde{Y}_t),Var(\beta_t|\beta_{t+1},\theta,\tilde{Y}_t)) $$
	$$ E(\beta_t|\beta_{t+1},\theta,\tilde{Y}_t)=\beta_{tt}+V_{tt}(V_{tt}+Q)^{-1}(\beta_{t+1}-\beta_{tt}) $$
	$$ Var(\beta_t|\beta_{t+1},\theta,\tilde{Y}_t)=V_{tt}-V_{tt}(V_{tt}+Q)^{-1} $$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Other Items of Note}
\begin{itemize}
	\item  Training Sample
	\begin{itemize}
		\item  Designating an early part of your data to estimate what your priors should be
	\end{itemize}
	\vspace{0.1 in}
	\item  Minnesota Prior
	\begin{itemize}
		\item  Rather than have the TE be AR(1), we can combine it with the Minnesota Prior:
	\end{itemize}
	\vspace{0.1 in}
	$$ \beta_t=A_0\beta_{t-1} + (I-A_0)\bar{\beta}_0+u_t $$
	\begin{itemize}
		\item  "A" is something we can set or we could add another block into our Gibbs sampler
	\end{itemize}
\end{itemize}
\end{frame}

\end{document}